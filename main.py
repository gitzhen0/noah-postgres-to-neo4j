#!/usr/bin/env python3
"""
NOAH PostgreSQL to Neo4j Converter - Main Entry Point

This is the main CLI interface for the NOAH database conversion tool.
"""

import sys
from pathlib import Path
import click
from rich.console import Console
from rich.table import Table as RichTable

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from noah_converter.utils.config import load_config
from noah_converter.utils.logger import setup_logger
from noah_converter.utils.db_connection import PostgreSQLConnection, Neo4jConnection
from noah_converter.schema_analyzer import SchemaAnalyzer
from noah_converter.mapping_engine import MappingEngine, CypherDDLGenerator, SpatialConfig

console = Console()


@click.group()
@click.option("--config", type=click.Path(exists=True), help="Path to config file")
@click.option("--verbose", is_flag=True, help="Enable verbose logging")
@click.pass_context
def cli(ctx, config, verbose):
    """NOAH PostgreSQL to Neo4j Converter CLI"""
    ctx.ensure_object(dict)

    # Load configuration
    config_path = Path(config) if config else None
    ctx.obj["config"] = load_config(config_path)

    # Setup logging
    log_level = "DEBUG" if verbose else ctx.obj["config"].logging.level
    setup_logger(
        log_file=ctx.obj["config"].logging.file,
        level=log_level,
        console=ctx.obj["config"].logging.console,
    )


@cli.command()
@click.option("--schema", default="public", help="PostgreSQL schema to analyze")
@click.option("--export", type=click.Path(), help="Export schema to JSON file")
@click.pass_context
def analyze(ctx, schema, export):
    """Analyze PostgreSQL database schema"""
    config = ctx.obj["config"]

    console.print("[bold blue]Starting schema analysis...[/bold blue]")

    # Connect to PostgreSQL
    pg_conn = PostgreSQLConnection(config.source_db)

    # Create analyzer
    analyzer = SchemaAnalyzer(pg_conn, config.schema_analyzer)

    # Analyze schema
    tables = analyzer.analyze(schema)

    # Display results
    _display_schema_summary(tables)

    # Export if requested
    if export:
        analyzer.export_schema(export)
        console.print(f"[green]âœ“[/green] Schema exported to: {export}")

    pg_conn.close()


@cli.command()
@click.option("--schema", default="public", help="PostgreSQL schema to analyze")
@click.option("--config-file", type=click.Path(), help="YAML config file (optional, auto-generates if not provided)")
@click.option("--output-dir", default="outputs/cypher", help="Output directory for generated files")
@click.pass_context
def generate_mapping(ctx, schema, config_file, output_dir):
    """Generate RDBMS to graph mapping"""
    config = ctx.obj["config"]

    console.print("[bold blue]ðŸ—ºï¸  Generating Graph Mapping...[/bold blue]\n")

    # Step 1: Analyze PostgreSQL schema
    console.print("[cyan]Step 1:[/cyan] Analyzing PostgreSQL schema...")
    pg_conn = PostgreSQLConnection(config.source_db)
    analyzer = SchemaAnalyzer(pg_conn, config.schema_analyzer)
    tables = analyzer.analyze(schema)
    console.print(f"[green]âœ“[/green] Found {len(tables)} tables\n")

    # Step 2: Generate graph schema
    console.print("[cyan]Step 2:[/cyan] Generating graph schema...")

    # PostGIS 100% é›¶æŸè€—è¿ç§»ï¼šæ‰€æœ‰ç©ºé—´æ•°æ®å…¨éƒ¨ä¿ç•™
    spatial_config = SpatialConfig(
        compute_centroids=True,
        compute_area=True,
        use_neo4j_point=True,
        preserve_wkt=True,
        preserve_geojson=True,
        compute_metrics=True,
        compute_bbox=True,
        neighbors_threshold_km=5.0
    )

    mapper = MappingEngine(
        tables=tables,
        spatial_config=spatial_config,
        config_file=config_file
    )

    graph_schema = mapper.generate_graph_schema()

    # Display summary
    summary = mapper.get_summary()
    console.print(f"[green]âœ“[/green] Generated graph schema:")
    console.print(f"  â€¢ Nodes: {summary['total_nodes']}")
    console.print(f"  â€¢ Relationships: {summary['total_relationships']}")
    console.print(f"  â€¢ Properties: {summary['total_properties']}")
    console.print(f"  â€¢ Spatial nodes: {summary['spatial_nodes']}")
    console.print(f"  â€¢ Spatial relationships: {summary['spatial_relationships']}\n")

    # Step 3: Export files
    console.print("[cyan]Step 3:[/cyan] Exporting schema files...")

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Export JSON schema
    json_file = output_path / "graph_schema.json"
    mapper.export_schema(str(json_file))

    # Export YAML config
    yaml_file = output_path / "mapping_config.yaml"
    mapper.export_yaml_config(str(yaml_file))

    # Export Cypher DDL
    constraints_file = output_path / "01_create_constraints.cypher"
    CypherDDLGenerator.export_constraints_script(graph_schema, str(constraints_file))

    indexes_file = output_path / "02_create_indexes.cypher"
    CypherDDLGenerator.export_indexes_script(graph_schema, str(indexes_file))

    # Export post-migration script (Neo4j Point + spatial indexes)
    post_migration_file = output_path / "03_post_migration_spatial.cypher"
    CypherDDLGenerator.export_post_migration_script(graph_schema, str(post_migration_file))

    console.print(f"\n[green]âœ… Mapping generation complete![/green]")
    console.print(f"[dim]Output directory: {output_dir}[/dim]")

    # Display node labels
    console.print("\n[bold]Generated Node Labels:[/bold]")
    for label in summary['node_labels']:
        console.print(f"  â€¢ {label}")

    pg_conn.close()


@cli.command()
@click.option("--schema", default="public", help="PostgreSQL schema to analyze")
@click.option("--hints", default="", help="Free-text context about the database domain")
@click.option(
    "--output",
    default="outputs/mapping_draft.yaml",
    type=click.Path(),
    help="Output path for the draft mapping YAML",
)
@click.option(
    "--save-raw",
    is_flag=True,
    help="Also save the raw LLM JSON response alongside the YAML",
)
@click.pass_context
def interpret(ctx, schema, hints, output, save_raw):
    """[Stage 2] Use LLM to analyze PostgreSQL schema and generate draft mapping_rules.yaml"""
    import os
    from rich.panel import Panel
    from rich.text import Text
    from noah_converter.schema_interpreter import SchemaInterpreter

    config = ctx.obj["config"]

    # Step 1: Analyze schema
    console.print("\n[bold blue]Stage 2: LLM Schema Interpreter[/bold blue]\n")
    console.print("[cyan]Step 1:[/cyan] Analyzing PostgreSQL schema...")

    pg_conn = PostgreSQLConnection(config.source_db)
    from noah_converter.utils.config import SchemaAnalyzerConfig
    analyzer = SchemaAnalyzer(pg_conn, config.schema_analyzer)
    tables = analyzer.analyze(schema)
    console.print(f"[green]âœ“[/green] Found {len(tables)} tables\n")

    # Step 2: Call LLM
    api_key = config.text2cypher.api_key or os.environ.get("ANTHROPIC_API_KEY")
    if not api_key or api_key.startswith("${"):
        console.print(
            "[red]âœ—[/red] ANTHROPIC_API_KEY not set. "
            "Export it or add to config.yaml under text2cypher.api_key"
        )
        return

    console.print(f"[cyan]Step 2:[/cyan] Calling LLM ({config.text2cypher.model})...")
    interpreter = SchemaInterpreter(
        api_key=api_key,
        model=config.text2cypher.model,
        max_tokens=6000,
    )
    result = interpreter.interpret(
        tables=tables,
        user_hints=hints,
        metadata={"database": config.source_db.database},
    )
    console.print(
        f"[green]âœ“[/green] LLM produced "
        f"{len(result.nodes)} nodes, "
        f"{len(result.relationships)} relationships, "
        f"{len(result.skipped_tables)} skipped tables\n"
    )

    # Step 3: Show summary with confidence levels
    _display_interpretation_summary(result)

    # Step 4: Validation warnings
    if result.validation_warnings:
        console.print("\n[bold yellow]âš  Validation Warnings:[/bold yellow]")
        for w in result.validation_warnings:
            console.print(f"  [yellow]â€¢[/yellow] {w}")

    # Step 5: Save output
    output_path = Path(output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(result.mapping_yaml)
    console.print(f"\n[green]âœ… Draft mapping saved:[/green] {output_path}")

    if save_raw:
        raw_path = output_path.with_suffix(".llm_response.json")
        raw_path.write_text(result.raw_llm_response)
        console.print(f"[dim]Raw LLM response: {raw_path}[/dim]")

    console.print(
        "\n[dim]Review the draft, then copy to config/mapping_rules.yaml "
        "and run: python main.py migrate[/dim]"
    )

    pg_conn.close()


def _display_interpretation_summary(result):
    """Display a confidence-colored summary of LLM decisions."""
    from rich.table import Table as RichTable

    CONF_COLOR = {"high": "green", "medium": "yellow", "low": "red"}

    # Nodes table
    node_table = RichTable(title="Proposed Node Types", show_header=True, header_style="bold")
    node_table.add_column("Label", style="cyan")
    node_table.add_column("Source Table", style="dim")
    node_table.add_column("Merge Key(s)", style="magenta")
    node_table.add_column("Confidence")
    node_table.add_column("Reasoning", max_width=50)

    for nd in result.nodes:
        color = CONF_COLOR.get(nd.confidence, "white")
        node_table.add_row(
            nd.label,
            nd.source_table,
            ", ".join(nd.merge_keys),
            f"[{color}]{nd.confidence}[/{color}]",
            nd.reasoning,
        )
    console.print(node_table)
    console.print()

    # Relationships table
    rel_table = RichTable(title="Proposed Relationships", show_header=True, header_style="bold")
    rel_table.add_column("Type", style="cyan")
    rel_table.add_column("From â†’ To", style="dim")
    rel_table.add_column("Source", style="dim")
    rel_table.add_column("Confidence")
    rel_table.add_column("Reasoning", max_width=45)

    for rd in result.relationships:
        color = CONF_COLOR.get(rd.confidence, "white")
        rel_table.add_row(
            rd.type,
            f"{rd.from_label} â†’ {rd.to_label}",
            rd.source_type,
            f"[{color}]{rd.confidence}[/{color}]",
            rd.reasoning,
        )
    console.print(rel_table)
    console.print()

    # Skipped tables
    if result.skipped_tables:
        console.print("[dim]Skipped tables:[/dim]")
        for s in result.skipped_tables:
            console.print(f"  [dim]â€¢ {s.table}: {s.reason}[/dim]")


@cli.command()
@click.option("--clear", is_flag=True, help="Clear Neo4j before migrating")
@click.option(
    "--mapping-rules",
    default="config/mapping_rules.yaml",
    type=click.Path(),
    help="Path to mapping_rules.yaml (default: config/mapping_rules.yaml)",
)
@click.pass_context
def migrate(ctx, clear, mapping_rules):
    """Migrate data from PostgreSQL to Neo4j (config-driven via mapping_rules.yaml)"""
    from noah_converter.data_migrator import GenericMigrator
    from noah_converter.mapping_engine.config import MappingConfigLoader

    config = ctx.obj["config"]

    if clear:
        console.print("[bold red]Warning: Neo4j will be cleared before migration[/bold red]")

    # Load graph schema from YAML
    rules_path = Path(mapping_rules)
    if not rules_path.exists():
        console.print(f"[red]âœ—[/red] Mapping rules not found: {rules_path}")
        return

    console.print(f"[bold blue]Starting NOAH data migration...[/bold blue]")
    console.print(f"[dim]Mapping rules: {rules_path}[/dim]")

    schema = MappingConfigLoader.load_graph_schema(str(rules_path))
    console.print(
        f"[green]âœ“[/green] Loaded schema: "
        f"{len(schema.nodes)} node types, {len(schema.relationships)} relationship types"
    )

    # Build connection params from config
    pg_dsn = {
        "host": config.source_db.host,
        "port": config.source_db.port,
        "dbname": config.source_db.database,
        "user": config.source_db.user,
        "password": config.source_db.password,
    }
    neo4j_uri = config.target_db.uri
    neo4j_auth = (config.target_db.user, config.target_db.password)

    migrator = GenericMigrator(
        pg_dsn=pg_dsn,
        neo4j_uri=neo4j_uri,
        neo4j_auth=neo4j_auth,
        schema=schema,
        batch_size=config.migration.batch_size,
    )
    result = migrator.migrate_all(clear=clear)
    migrator.close()

    total_nodes = sum(result["nodes"].values())
    total_rels  = sum(result["relationships"].values())
    console.print(f"\n[green]âœ… Migration complete![/green]")
    console.print(f"   Nodes created:         [bold]{total_nodes:,}[/bold]")
    console.print(f"   Relationships created: [bold]{total_rels:,}[/bold]")


@cli.command()
@click.option("--question", "-q", default=None, help="Natural language question (omit for interactive REPL)")
@click.option("--provider", default="claude", show_default=True, help="LLM provider: claude, openai, gemini")
@click.option("--model", default=None, help="Model ID (uses provider default if not set)")
@click.option("--no-execute", is_flag=True, help="Generate Cypher only, skip Neo4j execution")
@click.option("--no-explain", is_flag=True, help="Skip natural language explanation of results")
@click.pass_context
def query(ctx, question, provider, model, no_execute, no_explain):
    """Query the graph using natural language (Text2Cypher)"""
    import os
    from noah_converter.text2cypher import Text2CypherTranslator

    config = ctx.obj["config"]

    api_key = config.text2cypher.api_key or os.environ.get("ANTHROPIC_API_KEY")
    if not api_key or api_key.startswith("${"):
        console.print(
            "[red]âœ—[/red] ANTHROPIC_API_KEY not set. "
            "Export it or add to config.yaml under text2cypher.api_key"
        )
        return

    neo4j_conn = Neo4jConnection(config.target_db)

    try:
        console.print("[bold blue]Text2Cypher Query Interface[/bold blue]")
        console.print("[dim]Initializing schema context from Neo4j...[/dim]")

        translator = Text2CypherTranslator(
            neo4j_conn=neo4j_conn,
            llm_provider=provider,
            api_key=api_key,
            model=model or config.text2cypher.model,
        )

        console.print(f"[green]âœ“[/green] {translator.get_schema_summary()}")
        console.print()

        def run_query(q: str):
            with console.status("[cyan]Calling LLM to generate Cypher...[/cyan]"):
                result = translator.query(
                    question=q,
                    execute=not no_execute,
                    explain=not no_explain,
                )

            console.print("\n[bold]Generated Cypher:[/bold]")
            console.print(f"[cyan]{result['cypher']}[/cyan]")

            if result.get("error"):
                console.print(f"\n[red]Error:[/red] {result['error']}")
                return

            if result.get("results") is not None:
                records = result["results"]
                console.print(f"\n[bold]Results:[/bold] {len(records)} row(s)")
                if records:
                    cols = list(records[0].keys())
                    tbl = RichTable(show_header=True, header_style="bold magenta")
                    for col in cols:
                        tbl.add_column(col)
                    for row in records[:25]:
                        tbl.add_row(*[str(row.get(c, "")) for c in cols])
                    console.print(tbl)
                    if len(records) > 25:
                        console.print(f"[dim]... and {len(records) - 25} more rows[/dim]")

            if result.get("explanation"):
                console.print(f"\n[bold]Explanation:[/bold]")
                console.print(result["explanation"])

        if question:
            run_query(question)
        else:
            console.print("[dim]Interactive mode â€” type 'exit' or 'quit' to stop.[/dim]\n")
            while True:
                try:
                    q = click.prompt("Question", prompt_suffix=" > ")
                    if q.lower().strip() in ("exit", "quit", "q", ""):
                        break
                    run_query(q)
                    console.print()
                except (KeyboardInterrupt, EOFError):
                    break
            console.print("\n[dim]Goodbye![/dim]")

    finally:
        neo4j_conn.close()


@cli.command()
@click.option(
    "--mapping-rules",
    default="config/mapping_rules.yaml",
    type=click.Path(),
    help="Path to mapping_rules.yaml",
)
@click.option(
    "--sample-size",
    default=20,
    show_default=True,
    help="Rows to sample per node label for data verification",
)
@click.option(
    "--output",
    default="outputs/audit_report.json",
    type=click.Path(),
    help="Path for the audit report JSON",
)
@click.option("--no-coverage", is_flag=True, help="Skip property coverage check (faster)")
@click.pass_context
def audit(ctx, mapping_rules, sample_size, output, no_coverage):
    """Post-migration audit: count comparison, sample verification, coverage"""
    import json
    from noah_converter.data_auditor import MigrationAuditor
    from noah_converter.mapping_engine.config import MappingConfigLoader

    config = ctx.obj["config"]

    rules_path = Path(mapping_rules)
    if not rules_path.exists():
        console.print(f"[red]âœ—[/red] Mapping rules not found: {rules_path}")
        return

    schema = MappingConfigLoader.load_graph_schema(str(rules_path))
    console.print(
        f"[green]âœ“[/green] Loaded schema: "
        f"{len(schema.nodes)} node types, {len(schema.relationships)} relationship types"
    )

    pg_dsn = {
        "host": config.source_db.host,
        "port": config.source_db.port,
        "dbname": config.source_db.database,
        "user": config.source_db.user,
        "password": config.source_db.password,
    }
    neo4j_uri = config.target_db.uri
    neo4j_auth = (config.target_db.user, config.target_db.password)

    console.print("\n[bold blue]Running Post-Migration Audit...[/bold blue]\n")

    auditor = MigrationAuditor(
        pg_dsn=pg_dsn,
        neo4j_uri=neo4j_uri,
        neo4j_auth=neo4j_auth,
        schema=schema,
        sample_size=sample_size,
    )

    try:
        with console.status("[cyan]Auditing...[/cyan]"):
            report = auditor.run_audit(mapping_source=str(rules_path))
    finally:
        auditor.close()

    _display_audit_report(report, show_coverage=not no_coverage)

    # Save JSON report
    output_path = Path(output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(report.to_dict(), indent=2, default=str))
    console.print(f"\n[dim]Full report saved: {output_path}[/dim]")


def _display_audit_report(report, show_coverage: bool = True):
    """Render the audit report as Rich tables in the terminal."""
    from rich.panel import Panel

    # â”€â”€ Node count table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    node_table = RichTable(title="Node Count Verification", show_header=True, header_style="bold")
    node_table.add_column("Label", style="cyan")
    node_table.add_column("Source Table", style="dim")
    node_table.add_column("PostgreSQL", justify="right")
    node_table.add_column("Neo4j", justify="right")
    node_table.add_column("Diff", justify="right")
    node_table.add_column("Status")

    for nc in report.node_counts:
        if nc.match:
            status = "[green]âœ“ MATCH[/green]"
            diff_str = "â€”"
        else:
            status = "[red]âœ— MISMATCH[/red]"
            diff_str = f"[red]{nc.diff:+,}[/red]"
        node_table.add_row(
            nc.label,
            nc.source_table,
            f"{nc.pg_count:,}",
            f"{nc.neo4j_count:,}",
            diff_str,
            status,
        )
    console.print(node_table)
    console.print()

    # â”€â”€ Relationship count table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    rel_table = RichTable(title="Relationship Counts", show_header=True, header_style="bold")
    rel_table.add_column("Type", style="cyan")
    rel_table.add_column("Source", style="dim")
    rel_table.add_column("Neo4j", justify="right")
    rel_table.add_column("PG Expected", justify="right")
    rel_table.add_column("Status")

    for rc in report.rel_counts:
        if rc.neo4j_count == 0:
            status = "[red]âœ— EMPTY[/red]"
        elif not rc.match:
            status = "[yellow]âš  MISMATCH[/yellow]"
        else:
            status = "[green]âœ“[/green]"
        rel_table.add_row(
            rc.rel_type,
            rc.source_type,
            f"{rc.neo4j_count:,}",
            f"{rc.pg_expected:,}" if rc.pg_expected is not None else "N/A",
            status,
        )
    console.print(rel_table)
    console.print()

    # â”€â”€ Sample check table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if report.sample_checks:
        sample_table = RichTable(title="Data Sample Verification", show_header=True, header_style="bold")
        sample_table.add_column("Label", style="cyan")
        sample_table.add_column("Checked", justify="right")
        sample_table.add_column("Matched", justify="right")
        sample_table.add_column("Match Rate", justify="right")
        sample_table.add_column("Missing in PG", justify="right")
        sample_table.add_column("Status")

        for sc in report.sample_checks:
            rate = sc.match_rate
            if rate >= 95:
                status = "[green]âœ“[/green]"
                rate_str = f"[green]{rate:.1f}%[/green]"
            elif rate >= 80:
                status = "[yellow]âš [/yellow]"
                rate_str = f"[yellow]{rate:.1f}%[/yellow]"
            else:
                status = "[red]âœ—[/red]"
                rate_str = f"[red]{rate:.1f}%[/red]"
            sample_table.add_row(
                sc.label,
                str(sc.checked),
                str(sc.matched),
                rate_str,
                str(sc.missing_in_neo4j),
                status,
            )
        console.print(sample_table)

        # Show mismatch details if any
        all_mismatches = [
            (sc.label, m)
            for sc in report.sample_checks
            for m in sc.mismatches
        ]
        if all_mismatches:
            console.print("\n[bold yellow]Sample Mismatches (up to 10):[/bold yellow]")
            for label, m in all_mismatches[:10]:
                console.print(
                    f"  [dim]{label}[/dim] key={m.merge_key_value} "
                    f"[cyan]{m.property_name}[/cyan]: "
                    f"Neo4j=[yellow]{m.neo4j_value!r}[/yellow] "
                    f"PG=[blue]{m.pg_value!r}[/blue]"
                )
        console.print()

    # â”€â”€ Property coverage (optional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if show_coverage and report.property_coverage:
        for cov in report.property_coverage:
            low_coverage = [
                p for p in cov.properties if p.coverage_pct < 50
            ]
            if low_coverage:
                console.print(f"[dim]{cov.label}[/dim] â€” low-coverage properties:")
                for p in low_coverage:
                    console.print(
                        f"  [yellow]{p.property_name}[/yellow]: "
                        f"{p.coverage_pct:.0f}% ({p.populated:,}/{p.total_nodes:,})"
                    )
        console.print()

    # â”€â”€ Issues + overall status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if report.issues:
        console.print("[bold yellow]Issues Found:[/bold yellow]")
        for issue in report.issues:
            prefix = "[red]âœ—[/red]" if issue.startswith("ERROR") else "[yellow]âš [/yellow]"
            console.print(f"  {prefix} {issue}")
        console.print()

    color = {"PASS": "green", "WARN": "yellow", "FAIL": "red"}[report.overall_status]
    console.print(
        f"[bold {color}]Overall Status: {report.overall_status}[/bold {color}]"
    )


@cli.command()
@click.pass_context
def validate(ctx):
    """Validate migrated data"""
    console.print("[bold yellow]Use 'audit' command for data validation[/bold yellow]")


@cli.command()
@click.pass_context
def status(ctx):
    """Show connection status and database info"""
    config = ctx.obj["config"]

    console.print("[bold blue]Database Connection Status[/bold blue]\n")

    # Test PostgreSQL connection
    try:
        pg_conn = PostgreSQLConnection(config.source_db)
        tables = pg_conn.get_table_names()
        console.print(f"[green]âœ“[/green] PostgreSQL: Connected ({len(tables)} tables)")
        pg_conn.close()
    except Exception as e:
        console.print(f"[red]âœ—[/red] PostgreSQL: Connection failed - {e}")

    # Test Neo4j connection
    try:
        neo4j_conn = Neo4jConnection(config.target_db)
        node_count = neo4j_conn.get_node_count()
        rel_count = neo4j_conn.get_relationship_count()
        console.print(f"[green]âœ“[/green] Neo4j: Connected ({node_count} nodes, {rel_count} relationships)")
        neo4j_conn.close()
    except Exception as e:
        console.print(f"[red]âœ—[/red] Neo4j: Connection failed - {e}")


def _display_schema_summary(tables: dict):
    """Display schema analysis summary"""
    table = RichTable(title="Schema Analysis Summary")
    table.add_column("Table Name", style="cyan")
    table.add_column("Type", style="magenta")
    table.add_column("Columns", justify="right", style="green")
    table.add_column("Rows", justify="right", style="yellow")
    table.add_column("FKs", justify="right", style="blue")

    for table_name, table_obj in tables.items():
        table.add_row(
            table_name,
            table_obj.table_type.value,
            str(len(table_obj.columns)),
            str(table_obj.row_count or "N/A"),
            str(len(table_obj.foreign_keys)),
        )

    console.print(table)


if __name__ == "__main__":
    cli(obj={})
